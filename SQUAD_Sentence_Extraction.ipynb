{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SQUAD-Sentence-Extraction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/collarad/learning-bert/blob/master/SQUAD_Sentence_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2_TaWkHL2TG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/collarad/learning-bert.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd7RbLvQOaF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip /content/learning-bert/datasets/squad20/dev-v2.0.zip -d /content/learning-bert/datasets/squad20/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeZNj5FmbiCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%ls learning-bert/datasets/squad20/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7apnrBDPqJ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYZpHbQ9O-vw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZcN7sLLP1BO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0x3QEu1oEEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = '3. English law takes a dim view of stealing stuff from the shops. Some may argue that this is a pity.'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxK1QYISoFA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = nlp(text)\n",
        "for sent in doc.sents:\n",
        "    print(\"****\", sent.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Amo_LuidqtaF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/content/learning-bert/datasets/squad20/dev-v2.0.json\",'rb') as f:\n",
        "    j = json.load(f)\n",
        "\n",
        "numberOfData = len(j['data'])\n",
        "print(numberOfData)\n",
        "#paragraphs = j['data'][0]['paragraphs']\n",
        "    \n",
        "#TODO I do not like to many fors but due to time constraints this remains as a todo task\n",
        "for dataItem in j['data']:\n",
        "    print(dataItem['title'])\n",
        "    countErrors = 1\n",
        "    #Iterating Paragraphs in Data\n",
        "    for paragraph in dataItem['paragraphs']:\n",
        "      context = paragraph['context']\n",
        "      #print(len(context))\n",
        "      #Spliting the context in setences using spaCy\n",
        "      doc = nlp(context)\n",
        "      #Iterating all questions to find the sentence\n",
        "      for qa in paragraph['qas']:\n",
        "        try:\n",
        "          firstAnswer = qa['answers'][0]['text']\n",
        "          print(\"Answer: \"+firstAnswer)\n",
        "          for sent in doc.sents:\n",
        "            if firstAnswer in sent.text:\n",
        "              print(\"****\", sent.text)\n",
        "        except:\n",
        "          countErrors = countErrors + 1\n",
        "          #print('Error getting the answer text')\n",
        "        \n",
        "        \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}